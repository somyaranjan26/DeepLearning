{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sgan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somyaranjan26/DeepLearning/blob/master/SGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba8whvgfN76w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "DATADIR = \"\"\n",
        "\n",
        "CATEGORIES = [\"covid\", \"No_findings\",\"Pneumonia\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v1m3xi6d6bk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "6be8653f-8f80-4ee6-801b-ed47af4b98d6"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDWX_Fg7NO7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/cnnmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5PCDptgNqV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c3673163-da97-4e95-88ab-f5e7a5c84322"
      },
      "source": [
        "training_data = []\n",
        "IMG_SIZE = 28\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  \n",
        "\n",
        "        path = os.path.join(DATADIR,category) \n",
        "        class_num = CATEGORIES.index(category)  \n",
        "        for img in tqdm(os.listdir(path)): \n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE )\n",
        "\n",
        "                \n",
        "                \n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                new_array=new_array.reshape(-1)\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "            #except OSError as e:\n",
        "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "            #except Exception as e:\n",
        "            #    print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 796/796 [06:08<00:00,  2.16it/s]\n",
            "100%|██████████| 2731/2731 [15:22<00:00,  2.96it/s]\n",
            "100%|██████████| 3153/3153 [19:22<00:00,  2.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6680\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfKiwJqOOO8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(training_data)\n",
        "\n",
        "x1 = []\n",
        "y1 = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    x1.append(features)\n",
        "    \n",
        "    y1.append(label)\n",
        "\n",
        "\n",
        "x1 = np.array(x1)\n",
        "\n",
        "y1 = np.array(y1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuBChERXqN7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54854bef-1197-4664-cfad-c754ce33c594"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras import layers\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x1, y1, test_size = 0.2, random_state = 0)\n",
        "yTrain = keras.utils.to_categorical(yTrain,3)\n",
        "yTest = keras.utils.to_categorical(yTest, 3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAjJ9bWrbPGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x_height, x_width = [28, 28]\n",
        "num_channels = 1\n",
        "num_classes = 3\n",
        "latent_size = 100\n",
        "labeled_rate = 0.1\n",
        "\n",
        "log_path = './SSL_GAN_logs_final.csv'\n",
        "model_path ='./SSL_GAN_models_final.ckpt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW30wjbUscjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJDxEhd0pilE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def D(x, dropout_rate, is_training, reuse = True, print_summary = True):\n",
        "    # discriminator (x -> n + 1 class)\n",
        "\n",
        "    with tf.variable_scope('Discriminator', reuse = reuse) as scope:\n",
        "        # layer1 - do not use Batch Normalization on the first layer of Discriminator\n",
        "        conv1 = tf.layers.conv2d(x, 32, [5, 5],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same')\n",
        "        lrelu1 = tf.maximum(0.2 * conv1, conv1) #leaky relu\n",
        "        dropout1 = tf.layers.dropout(lrelu1, dropout_rate)\n",
        "\n",
        "        # layer2\n",
        "        conv2 = tf.layers.conv2d(dropout1, 64, [3, 3],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same')\n",
        "        batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training)\n",
        "        lrelu2 = tf.maximum(0.2 * batch_norm2, batch_norm2)\n",
        "\n",
        "        # layer3\n",
        "        conv3 = tf.layers.conv2d(lrelu2, 128, [2, 2],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same')\n",
        "        batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
        "        lrelu3 = tf.maximum(0.2 * batch_norm3, batch_norm3)\n",
        "        dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
        "\n",
        "        # layer 4\n",
        "        conv4 = tf.layers.conv2d(dropout3, 128, [2, 2],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same')\n",
        "        # do not use batch_normalization on this layer - next layer, \"flatten5\",\n",
        "        # will be used for \"Feature Matching\"\n",
        "        lrelu4 = tf.maximum(0.2 * conv4, conv4)\n",
        "        \n",
        "        # layer 5\n",
        "        flatten_length = lrelu4.get_shape().as_list()[1] * \\\n",
        "                         lrelu4.get_shape().as_list()[2] * lrelu4.get_shape().as_list()[3]\n",
        "        flatten5 = tf.reshape(lrelu4, (-1, flatten_length)) # used for \"Feature Matching\" \n",
        "        fc5 = tf.layers.dense(flatten5, (num_classes + 1))\n",
        "        output = tf.nn.softmax(fc5)\n",
        "\n",
        "        assert output.get_shape()[1:] == [num_classes + 1]\n",
        "\n",
        "        if print_summary:\n",
        "            print('Discriminator summary:\\n x: %s\\n' \\\n",
        "                  ' D1: %s\\n D2: %s\\n D3: %s\\n D4: %s\\n' %(x.get_shape(), \n",
        "                                                           dropout1.get_shape(),\n",
        "                                                           lrelu2.get_shape(), \n",
        "                                                           dropout3.get_shape(),\n",
        "                                                           lrelu4.get_shape()))\n",
        "        return flatten5, fc5, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETNrQIqQpurf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def G(z, is_training, reuse = False, print_summary = False):\n",
        "    # generator (z -> x)\n",
        "\n",
        "    with tf.variable_scope('Generator', reuse = reuse) as scope:\n",
        "        # layer 0\n",
        "        z_reshaped = tf.reshape(z, [-1, 1, 1, latent_size])\n",
        "\n",
        "        # layer 1\n",
        "        deconv1 = tf.layers.conv2d_transpose(z_reshaped,\n",
        "                                             filters = latent_size,\n",
        "                                             kernel_size = [2, 2],\n",
        "                                             strides = [1, 1],\n",
        "                                             padding = 'valid')\n",
        "        batch_norm1 = tf.layers.batch_normalization(deconv1, training = is_training)\n",
        "        relu1 = tf.nn.relu(batch_norm1)\n",
        "\n",
        "        # layer 2\n",
        "        deconv2 = tf.layers.conv2d_transpose(relu1,\n",
        "                                             filters = 64,\n",
        "                                             kernel_size = [3, 3],\n",
        "                                             strides = [2, 2],\n",
        "                                             padding = 'valid')\n",
        "        batch_norm2 = tf.layers.batch_normalization(deconv2, training = is_training)\n",
        "        relu2 = tf.nn.relu(batch_norm2)\n",
        "\n",
        "        # layer 3\n",
        "        deconv3 = tf.layers.conv2d_transpose(relu2,\n",
        "                                             filters = 32,\n",
        "                                             kernel_size = [4, 4],\n",
        "                                             strides = [2, 2],\n",
        "                                             padding = 'valid')\n",
        "        batch_norm3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
        "        relu3 = tf.nn.relu(batch_norm3)\n",
        "\n",
        "        # layer 4 - do not use Batch Normalization on the last layer of Generator\n",
        "        deconv4 = tf.layers.conv2d_transpose(relu3,\n",
        "                                             filters = num_channels,\n",
        "                                             kernel_size = [6, 6],\n",
        "                                             strides = [2, 2],\n",
        "                                             padding = 'valid')\n",
        "        tanh4 = tf.tanh(deconv4)\n",
        "\n",
        "        assert tanh4.get_shape()[1:] == [x_height, x_width, num_channels]\n",
        "        if print_summary:\n",
        "            print('Generator summary:\\n z: %s\\n' \\\n",
        "                  ' G0: %s\\n G1: %s\\n G2: %s\\n G3: %s\\n G4: %s\\n' %(z.get_shape(),\n",
        "                                                                    z_reshaped.get_shape(),\n",
        "                                                                    relu1.get_shape(),\n",
        "                                                                    relu2.get_shape(),\n",
        "                                                                    relu3.get_shape(),\n",
        "                                                                    tanh4.get_shape()))\n",
        "        return tanh4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a0o2J7-pxMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(x_real, z, label, dropout_rate, is_training, print_summary = False):\n",
        "    # build model\n",
        "    D_real_features, D_real_logit, D_real_prob = D(x_real, dropout_rate, is_training,\n",
        "                                                   reuse = False, print_summary = print_summary)\n",
        "    x_fake = G(z, is_training, reuse = False, print_summary = print_summary)\n",
        "    D_fake_features, D_fake_logit, D_fake_prob = D(x_fake, dropout_rate, is_training,\n",
        "                                                   reuse = True, print_summary = print_summary)\n",
        "\n",
        "    return D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, x_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voe9Iw5cpzwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def prepare_labels(label):\n",
        "    # add extra label for fake data\n",
        "    extended_label = tf.concat([label, tf.zeros([tf.shape(label)[0], 1])], axis = 1)\n",
        "\n",
        "\n",
        "    return extended_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vctiE868p1Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
        "                  D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
        "    epsilon = 1e-8 # used to avoid NAN loss\n",
        "    # *** Discriminator loss ***\n",
        "    # supervised loss\n",
        "    # which class the real data belongs to\n",
        "    tmp = tf.nn.softmax_cross_entropy_with_logits(logits = D_real_logit,\n",
        "                                                  labels = extended_label)\n",
        "    D_L_supervised = tf.reduce_sum(labeled_mask * tmp) / tf.reduce_sum(labeled_mask) # to ignore\n",
        "                                                                                     # unlabeled\n",
        "                                                                                     # data\n",
        "\n",
        "    # unsupervised loss\n",
        "    # data is real\n",
        "    prob_real_be_real = 1 - D_real_prob[:, -1] + epsilon\n",
        "    tmp_log = tf.log(prob_real_be_real)\n",
        "    D_L_unsupervised1 = -1 * tf.reduce_mean(tmp_log)\n",
        "\n",
        "    # data is fake\n",
        "    prob_fake_be_fake = D_fake_prob[:, -1] + epsilon\n",
        "    tmp_log = tf.log(prob_fake_be_fake)\n",
        "    D_L_unsupervised2 = -1 * tf.reduce_mean(tmp_log)\n",
        "\n",
        "    D_L = D_L_supervised + D_L_unsupervised1 + D_L_unsupervised2\n",
        "\n",
        "    # *** Generator loss ***\n",
        "    # fake data is mistaken to be real\n",
        "    prob_fake_be_real = 1 - D_fake_prob[:, -1] + epsilon\n",
        "    tmp_log =  tf.log(prob_fake_be_real)\n",
        "    G_L1 = -1 * tf.reduce_mean(tmp_log)\n",
        "\n",
        "    # Feature Maching\n",
        "    tmp1 = tf.reduce_mean(D_real_features, axis = 0)\n",
        "    tmp2 = tf.reduce_mean(D_fake_features, axis = 0)\n",
        "    G_L2 = tf.reduce_mean(tf.square(tmp1 - tmp2))\n",
        "\n",
        "    G_L = G_L1 + G_L2\n",
        "\n",
        "    # accuracy\n",
        "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),\n",
        "                                  tf.argmax(extended_label[:, :-1], 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    return D_L_supervised, D_L_unsupervised1, D_L_unsupervised2, D_L, G_L, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akbPQeRLp2jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimizer(D_Loss, G_Loss, D_learning_rate, G_learning_rate):\n",
        "    # D and G optimizer\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        all_vars = tf.trainable_variables()\n",
        "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
        "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
        "\n",
        "        D_optimizer = tf.train.AdamOptimizer(D_learning_rate).minimize(D_Loss, var_list = D_vars)\n",
        "        G_optimizer = tf.train.AdamOptimizer(G_learning_rate).minimize(G_Loss, var_list = G_vars)\n",
        "        return D_optimizer, G_optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gsjUC4wp3sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_fake_data(data, grid_size = [5, 5]):\n",
        "    # visualize some data generated by G\n",
        "    _, axes = plt.subplots(figsize = grid_size, nrows = grid_size[0], ncols = grid_size[1],\n",
        "                           sharey = True, sharex = True)\n",
        "\n",
        "    size = grid_size[0] * grid_size[1]\n",
        "    index = np.int_(np.random.uniform(0, data.shape[0], size = (size)))\n",
        "    figs = data[index].reshape(-1, x_height, x_width)\n",
        "\n",
        "    for idx, ax in enumerate(axes.flatten()):\n",
        "        ax.axis('off')\n",
        "        ax.imshow(figs[idx], cmap = 'gray')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O2uzxvTp4pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model_on_imporvemnet(file_path, sess, cv_acc, cv_accs):\n",
        "  #  # save model when there is improvemnet in cv_acc value\n",
        "    if cv_accs == [] or cv_acc > np.max(cv_accs):\n",
        "        saver = tf.train.Saver(max_to_keep = 1)\n",
        "        saver.save(sess, file_path)\n",
        "        print('Model saved')\n",
        "    print('')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_i4RV3Pp54L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labled_mask(labeled_rate, batch_size):\n",
        "    # get labeled mask to mask some data unlabeled\n",
        "    labeled_mask = np.zeros([batch_size], dtype = np.float32)\n",
        "    labeled_count = np.int(batch_size * labeled_rate)\n",
        "    labeled_mask[range(labeled_count)] = 1.0\n",
        "    #np.random.shuffle(labeled_mask)\n",
        "\n",
        "    return labeled_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBF-ibK6p6yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_loss_acc(file_path, epoch, train_loss_D, train_loss_G, train_Acc,\n",
        "                 cv_loss_D, cv_loss_G, cv_Acc, log_mode = 'a'):\n",
        "    # log train and cv losses as well as accuracy\n",
        "    mode = log_mode if epoch == 0 else 'a'\n",
        "\n",
        "    with open(file_path, mode) as f:\n",
        "        if mode == 'w':\n",
        "            header = 'epoch, train_loss_D, train_loss_G,' \\\n",
        "                     'train_Acc, cv_loss_D, cv_loss_G, cv_Acc\\n'\n",
        "            f.write(header)\n",
        "\n",
        "        line = '%d, %f, %f, %f, %f, %f, %f\\n' %(epoch, train_loss_D, train_loss_G, train_Acc,\n",
        "                                                cv_loss_D, cv_loss_G, cv_Acc)\n",
        "        f.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-hanF3-aPLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x):\n",
        "    # normalize data\n",
        "    x = (x - 127.5) / 127.5\n",
        "    return x.reshape((-1, x_height, x_width, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfhWKU3Op9L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_SSL_GAN(batch_size, epochs):\n",
        "    # train Semi-Supervised Learning GAN\n",
        "    train_D_losses, train_G_losses, train_Accs = [], [], []\n",
        "    cv_D_losses, cv_G_losses, cv_Accs = [], [], []\n",
        "\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    x = tf.placeholder(tf.float32, name = 'x', shape = [None, x_height, x_width, num_channels])\n",
        "    label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes])\n",
        "    labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
        "    z = tf.placeholder(tf.float32, name = 'z', shape = [None, latent_size])\n",
        "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
        "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
        "    G_learning_rate = tf.placeholder(tf.float32, name = 'G_learning_rate')\n",
        "    D_learning_rate = tf.placeholder(tf.float32, name = 'D_learning_rate')\n",
        "\n",
        "    model = build_model(x, z, label, dropout_rate, is_training, print_summary = True)\n",
        "    D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, fake_data = model\n",
        "    extended_label = prepare_labels(label)\n",
        "    loss_acc  = loss_accuracy(D_real_features, D_real_logit, D_real_prob,\n",
        "                              D_fake_features, D_fake_logit, D_fake_prob,\n",
        "                              extended_label, labeled_mask)\n",
        "    _, _, _, D_L, G_L, accuracy = loss_acc\n",
        "    D_optimizer, G_optimizer = optimizer(D_L, G_L, G_learning_rate, D_learning_rate)\n",
        "\n",
        "    print('training....')\n",
        "\n",
        "    with tf.Session() as sess:       \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            t_total = 0\n",
        "            for iter in range(int(xTrain.shape[0] / batch_size) + 1):\n",
        "                t_start = time.time()\n",
        "                batch = next_batch(batch_size,xTrain,yTrain)\n",
        "                batch_z = np.random.uniform(-1.0, 1.0, size = (batch_size, latent_size))\n",
        "                mask = get_labled_mask(labeled_rate, batch_size)\n",
        "                train_feed_dictionary = {x:  normalize(batch[0]),\n",
        "                                         z: batch_z,\n",
        "                                         label: batch[1],\n",
        "                                         labeled_mask: mask,\n",
        "                                         dropout_rate: 0.5,\n",
        "                                         G_learning_rate: 1e-5,\n",
        "                                         D_learning_rate: 1e-5,\n",
        "                                         is_training: True}\n",
        "\n",
        "                D_optimizer.run(feed_dict = train_feed_dictionary)\n",
        "                G_optimizer.run(feed_dict = train_feed_dictionary)\n",
        "\n",
        "                train_D_loss = D_L.eval(feed_dict = train_feed_dictionary)\n",
        "                train_G_loss = G_L.eval(feed_dict = train_feed_dictionary)\n",
        "                train_accuracy = accuracy.eval(feed_dict = train_feed_dictionary)\n",
        "                t_total += (time.time() - t_start)\n",
        "\n",
        "                print('epoch: %d, time: %f | train_G_Loss: %f, ' \\\n",
        "                      'train_D_+= (timeloss: %f, train_acc: %f' %(epoch, t_total,\n",
        "                                                          train_G_loss, train_D_loss,\n",
        "                                                          train_accuracy))\n",
        "            train_D_losses.append(train_D_loss)\n",
        "            train_G_losses.append(train_G_loss)\n",
        "            train_Accs.append(train_accuracy)\n",
        "\n",
        "            # Cross-Validation\n",
        "            cv_size = yTest.shape[0]\n",
        "            cv_batch_z = np.random.uniform(-1.0, 1.0, size = (cv_size, latent_size))\n",
        "            mask = get_labled_mask(1, cv_size)\n",
        "            cv_feed_dictionary = {x: normalize(xTest),\n",
        "                                  z: cv_batch_z,\n",
        "                                  label: yTest,\n",
        "                                  labeled_mask: mask,\n",
        "                                  dropout_rate: 0.0,\n",
        "                                  is_training: False}\n",
        "\n",
        "            cv_D_loss = D_L.eval(feed_dict = cv_feed_dictionary)\n",
        "            cv_G_loss = G_L.eval(feed_dict = cv_feed_dictionary)\n",
        "            cv_accuracy = accuracy.eval(feed_dict = cv_feed_dictionary)\n",
        "\n",
        "            log_loss_acc(log_path, epoch, train_D_loss, train_G_loss, train_accuracy,\n",
        "                         cv_D_loss, cv_G_loss, cv_accuracy, log_mode = 'w')\n",
        "            print('\\ncv_G_Loss: %f, cv_D_loss: %f, cv_acc: %f' %(cv_G_loss,\n",
        "                                                                 cv_D_loss,\n",
        "                                                                 cv_accuracy))\n",
        "            save_model_on_imporvemnet(model_path, sess, cv_accuracy, cv_Accs)\n",
        "            cv_D_losses.append(cv_D_loss)\n",
        "            cv_G_losses.append(cv_G_loss)\n",
        "            cv_Accs.append(cv_accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                fakes = fake_data.eval(feed_dict = cv_feed_dictionary)\n",
        "                plot_fake_data(fakes, [5, 5])\n",
        "\n",
        "    return train_D_losses, train_G_losses, train_Accs, cv_D_losses, cv_G_losses, cv_Accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16v2GbbzqBR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "730884c0-3074-402b-f04f-0849be7820f3"
      },
      "source": [
        "loss_acc = train_SSL_GAN(512, 1001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9a341e57267b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_SSL_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_SSL_GAN' is not defined"
          ]
        }
      ]
    }
  ]
}